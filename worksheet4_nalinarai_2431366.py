# -*- coding: utf-8 -*-
"""Worksheet4_nalinaRai_2431366.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OxfoZdQMCa9-3OBQYpH1IXSD9GiwuVzn
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import time
import matplotlib.pyplot as plt

# Loading the dataset
dataset_path = '/content/drive/MyDrive/Worksheet-4/diabetes.csv'
data = pd.read_csv(dataset_path)

#Problem - 1: Perform a classification task with knn from scratch.
#1. Load the Dataset:
# Read the dataset into a pandas DataFrame.
# Display the first few rows and perform exploratory data analysis (EDA) to understand the dataset (e.g., check data types, missing values, summary statistics).

def perform_eda(data):
    print("First few rows:")
    print(data.head())
    print("\nDataset Info:")
    print(data.info())
    print("\nSummary Statistics:")
    print(data.describe())
    print("\nMissing Values:")
    print(data.isnull().sum())

perform_eda(data)

# 2. Handle Missing Data:
#    - Handle any missing values appropriately, either by dropping or imputing them based on the data.
data.fillna(data.median(), inplace=True)  # Impute missing values with median

# 3. Feature Engineering:
#    - Separate the feature matrix (X) and target variable (y).
#    - Perform a train-test split from scratch using a 70%-30% ratio.

X = data.iloc[:, :-1].values  # Features
y = data.iloc[:, -1].values   # Target

def train_test_split(X, y, test_size=0.3, random_state=42):
    np.random.seed(random_state)
    indices = np.random.permutation(len(X))
    split_idx = int(len(X) * (1 - test_size))
    train_idx, test_idx = indices[:split_idx], indices[split_idx:]
    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# 4. Implement KNN:
#    - Build the KNN algorithm from scratch (no libraries like scikit-learn for KNN).
#    - Compute distances using Euclidean distance.
#    - Write functions for:
#        * Predicting the class for a single query.
#        * Predicting classes for all test samples.
#    - Evaluate the performance using accuracy.

def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))

def knn_predict_single(query, X_train, y_train, k):
    distances = [euclidean_distance(query, x_train) for x_train in X_train]
    neighbors_idx = np.argsort(distances)[:k]
    neighbors_labels = y_train[neighbors_idx]
    return np.bincount(neighbors_labels).argmax()

def knn_predict(X_test, X_train, y_train, k):
    return np.array([knn_predict_single(query, X_train, y_train, k) for query in X_test])

# Evaluating the model with k=3
k = 3
y_pred = knn_predict(X_test, X_train, y_train, k)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy with k={k}: {accuracy}")

# Problem - 2: Experimentation
# 1. Repeat the Classification Task:
#    - Scale the Feature matrix X.
#    - Use the scaled data for training and testing the KNN Classifier.
#    - Record the results.

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

y_pred_scaled = knn_predict(X_test_scaled, X_train_scaled, y_train, k)
accuracy_scaled = accuracy_score(y_test, y_pred_scaled)

print(f"Accuracy with k={k} (scaled features): {accuracy_scaled}")

# 2. Comparative Analysis:
#    - Compare the accuracy and performance of the KNN model on the original dataset from Problem 1
#      versus the scaled dataset.
#    - Discuss:
#        * How scaling impacted the KNN performance.
#        * The reason for any observed changes in accuracy.

# Problem - 3: Experimentation with k
# 1. Vary the number of neighbors - k:
#    - Run the KNN model on both the original and scaled datasets for a range of:
#        k = 1, 2, 3, ..., 15
#    - For each k, record:
#        * Accuracy.
#        * Time taken to make predictions.

k_values = range(1, 16)
original_accuracies = []
scaled_accuracies = []
original_times = []
scaled_times = []

for k in k_values:
    start = time.time()
    y_pred = knn_predict(X_test, X_train, y_train, k)
    original_times.append(time.time() - start)
    original_accuracies.append(accuracy_score(y_test, y_pred))

    start = time.time()
    y_pred_scaled = knn_predict(X_test_scaled, X_train_scaled, y_train, k)
    scaled_times.append(time.time() - start)
    scaled_accuracies.append(accuracy_score(y_test, y_pred_scaled))

# 2. Visualize the Results:
#    - Plot the following graphs:
#        * k vs. Accuracy for original and scaled datasets.
#        * k vs. Time Taken for original and scaled datasets.

plt.figure(figsize=(12, 6))

# 2. Visualize the Results:
#    - Plot the following graphs:
#        * k vs. Accuracy for original and scaled datasets.
#        * k vs. Time Taken for original and scaled datasets.

plt.figure(figsize=(12, 6))

# Accuracy vs k
plt.subplot(1, 2, 1)
plt.plot(k_values, original_accuracies, label='Original Features', marker='o')
plt.plot(k_values, scaled_accuracies, label='Scaled Features', marker='o')
plt.title('k vs Accuracy')
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.legend()

# Time vs k
plt.subplot(1, 2, 2)
plt.plot(k_values, original_times, label='Original Features', marker='o')
plt.plot(k_values, scaled_times, label='Scaled Features', marker='o')
plt.title('k vs Time Taken')
plt.xlabel('k')
plt.ylabel('Time Taken (s)')
plt.legend()

plt.tight_layout()
plt.show()

# 3. Analyze and Discuss:
#    - Discuss how the choice of k affects the accuracy and computational cost.
#    - Identify the optimal k based on your analysis.

# Discussion
print("\nDiscussion:")
print("1. Scaling the features significantly impacted the performance of KNN. After scaling, the accuracy improved for most values of k, and computational cost decreased.")
print("2. The choice of k directly impacts both accuracy and time taken. Lower k tends to overfit, while higher k may underfit.")
print("3. Optimal k based on the analysis: k=")

"""Challenges of k-NN:

Slow for large datasets due to distance calculations with all points.
Curse of Dimensionality: High dimensions reduce distance effectiveness.
Scalability: Time complexity grows with dataset size.
Sensitive to noise and outliers.

Efficiency Improvements:

Approximate Nearest Neighbors: Use methods like LSH or KD-Trees.
Dimensionality Reduction: Apply PCA for faster computation.
Efficient Data Structures: Use KD-Trees or Ball-Trees.
Parallelization/GPU: Speed up calculations.
Weighted Voting: Give closer neighbors more influence.
"""